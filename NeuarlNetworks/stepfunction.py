'''

1) Weights :
    Weights control the signal (or the strength of the connection) between two neurons.
    In other words, a weight decides how much influence the input will have on the output. Biases, which are constant,
    are an additional input into the next layer that will always have the value of 1.

2) Bias :
    Bias allows you to shift the activation function by adding a constant (i.e. the given bias) to the input. 
    Bias in Neural Networks can be thought of as analogous to the role of a constant in a linear function,
    whereby the line is effectively transposed by the constant value.

'''




# stepfunction  :                                                                                                                                                                                         
    # if  "input > 0 " => output = 1                                                                                                                                                    
    # else if (input < 0) => output = 0                                                                                                                                     
    # output will be alwase 0 or 1                                                       
# how to use stepfunction as activation function :                                                                                                                                  
    # input* weight + baised                                                                                                                                            
    #                                                                                                                       



# Activation Function
'''
1) sigmoid activation function(logistic function):
    The input to the function is transformed into a value between 0.0 and 1.0.
    

'''













